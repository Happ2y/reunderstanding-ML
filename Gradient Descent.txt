An algorithm where we try to global minimum of a given function using it's derivative.
    - Batch gradient descent : All the dataset values are fed at once
    - Mini Batch gradient descent : Data Scientist decides what number of dataset values are to be fed at one go
    - Stochastic gradient descent : 1 dataset value is fed at once (episodic)